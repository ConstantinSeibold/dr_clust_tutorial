{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Image Analysis Tutorial: Dimensionality Reduction, Clustering & Outlier Detection\n",
    "\n",
    "This tutorial demonstrates various techniques for analyzing medical image embeddings using the SLAKE dataset.\n",
    "\n",
    "## Overview\n",
    "1. **Dataset**: SLAKE medical VQA dataset\n",
    "2. **Embeddings**: BiomedCLIP (medical vision-language model)\n",
    "3. **Clustering**: DBSCAN, HDBSCAN, K-means, FINCH\n",
    "4. **Dimensionality Reduction**: PCA, t-SNE, UMAP, h-NNE\n",
    "5. **Visualization**: Comparative analysis of methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "Install all required packages for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q open-clip-torch torch pillow numpy scikit-learn\n",
    "!pip install -q hdbscan umap-learn hnne\n",
    "!pip install -q matplotlib seaborn\n",
    "!pip install -q gdown  # For Google Drive downloads\n",
    "!pip install -q huggingface_hub  # For downloading from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/manifold/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import open_clip\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "import hdbscan\n",
    "try:\n",
    "    from finch import FINCH\n",
    "except ImportError:\n",
    "    !pip install -q finch-clust\n",
    "    from finch import FINCH\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from hnne import HNNE\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Download & Loading\n",
    "\n",
    "Download the SLAKE dataset from Google Drive and parse its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists!\n"
     ]
    }
   ],
   "source": [
    "# Download SLAKE dataset from Google Drive\n",
    "# Note: You may need to manually download if gdown fails due to Google Drive restrictions\n",
    "# Link: https://drive.google.com/file/d/1EZ0WpO5Z6BJUqC3iPBQJJS1INWSMsh7U/view\n",
    "\n",
    "dataset_path = Path('./Slake1.0')\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    print(\"Downloading SLAKE dataset...\")\n",
    "    !gdown --fuzzy 'https://drive.google.com/file/d/1EZ0WpO5Z6BJUqC3iPBQJJS1INWSMsh7U/view'\n",
    "    \n",
    "    # Extract the archive\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile('Slake1.0.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "    print(\"Dataset downloaded and extracted!\")\n",
    "else:\n",
    "    print(\"Dataset already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 642 samples from SLAKE dataset\n",
      "\n",
      "Sample data structure:\n",
      "Sample ID: xmlab0\n",
      "Image path: Slake1.0/imgs/xmlab0/source.jpg\n",
      "Number of QA pairs: 20\n"
     ]
    }
   ],
   "source": [
    "# Parse dataset structure\n",
    "def load_slake_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Load SLAKE dataset structure.\n",
    "    Returns: List of dicts with image_path, qa_pairs, and sample_id\n",
    "    \"\"\"\n",
    "    imgs_path = dataset_path / 'imgs'\n",
    "    samples = []\n",
    "    \n",
    "    for sample_dir in sorted(imgs_path.iterdir()):\n",
    "        if not sample_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        source_img = sample_dir / 'source.jpg'\n",
    "        question_file = sample_dir / 'question.json'\n",
    "        \n",
    "        if source_img.exists() and question_file.exists():\n",
    "            with open(question_file, 'r') as f:\n",
    "                qa_data = json.load(f)\n",
    "            \n",
    "            samples.append({\n",
    "                'sample_id': sample_dir.name,\n",
    "                'image_path': str(source_img),\n",
    "                'qa_pairs': qa_data\n",
    "            })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Load dataset\n",
    "slake_data = load_slake_dataset(dataset_path)\n",
    "print(f\"Loaded {len(slake_data)} samples from SLAKE dataset\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample data structure:\")\n",
    "print(f\"Sample ID: {slake_data[0]['sample_id']}\")\n",
    "print(f\"Image path: {slake_data[0]['image_path']}\")\n",
    "print(f\"Number of QA pairs: {len(slake_data[0]['qa_pairs'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Generation with BiomedCLIP\n",
    "\n",
    "We'll use the BiomedCLIP model to generate embeddings for:\n",
    "- **Images**: Using the vision encoder\n",
    "- **Text (QA pairs)**: Using the text encoder\n",
    "\n",
    "Embeddings will be saved as numpy files for quick reloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BiomedCLIP model...\n",
      "Model loaded successfully!\n",
      "Model architecture: CustomTextCLIP\n",
      "Image input size: (224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Load BiomedCLIP model using open_clip\n",
    "print(\"Loading BiomedCLIP model...\")\n",
    "\n",
    "# Download model files from HuggingFace\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\",\n",
    "    filename=\"open_clip_pytorch_model.bin\"\n",
    ")\n",
    "\n",
    "# Load model and preprocessing\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "tokenizer = open_clip.get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model architecture: {model.__class__.__name__}\")\n",
    "print(f\"Image input size: {model.visual.image_size if hasattr(model.visual, 'image_size') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for embeddings\n",
    "embeddings_dir = Path('./embeddings')\n",
    "embeddings_dir.mkdir(exist_ok=True)\n",
    "\n",
    "image_embeddings_path = embeddings_dir / 'image_embeddings.npy'\n",
    "text_embeddings_path = embeddings_dir / 'text_embeddings.npy'\n",
    "metadata_path = embeddings_dir / 'metadata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating image embeddings: 100%|██████████| 21/21 [00:42<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embeddings saved to embeddings/image_embeddings.npy\n",
      "Image embeddings shape: (642, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate image embeddings\n",
    "def generate_image_embeddings(data, model, preprocess, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate embeddings for all images in the dataset using open_clip.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(data), batch_size), desc=\"Generating image embeddings\"):\n",
    "            batch = data[i:i+batch_size]\n",
    "            \n",
    "            # Load and process images\n",
    "            images = []\n",
    "            for sample in batch:\n",
    "                img = Image.open(sample['image_path']).convert('RGB')\n",
    "                img_tensor = preprocess(img)\n",
    "                images.append(img_tensor)\n",
    "            \n",
    "            # Stack images into batch\n",
    "            images_batch = torch.stack(images).to(device)\n",
    "            \n",
    "            # Get image embeddings\n",
    "            image_features = model.encode_image(images_batch)\n",
    "            # Normalize embeddings\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            embeddings.append(image_features.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Generate or load image embeddings\n",
    "if image_embeddings_path.exists():\n",
    "    print(\"Loading existing image embeddings...\")\n",
    "    image_embeddings = np.load(image_embeddings_path)\n",
    "else:\n",
    "    print(\"Generating image embeddings...\")\n",
    "    image_embeddings = generate_image_embeddings(slake_data, model, preprocess_val, device)\n",
    "    np.save(image_embeddings_path, image_embeddings)\n",
    "    print(f\"Image embeddings saved to {image_embeddings_path}\")\n",
    "\n",
    "print(f\"Image embeddings shape: {image_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text embeddings:  69%|██████▉   | 304/439 [20:04<10:42,  4.76s/it]"
     ]
    }
   ],
   "source": [
    "# Generate text embeddings from QA pairs\n",
    "def generate_text_embeddings(data, model, tokenizer, device, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate embeddings for all QA pairs in the dataset using open_clip.\n",
    "    Each image may have multiple QA pairs - we'll create separate embeddings.\n",
    "    \"\"\"\n",
    "    all_texts = []\n",
    "    text_to_image_map = []  # Maps text embedding index to image index\n",
    "    \n",
    "    # Collect all QA pairs\n",
    "    for img_idx, sample in enumerate(data):\n",
    "        for qa in sample['qa_pairs']:\n",
    "            question = qa.get('question', '')\n",
    "            answer = qa.get('answer', '')\n",
    "            # Combine question and answer\n",
    "            text = f\"{question} {answer}\"\n",
    "            all_texts.append(text)\n",
    "            text_to_image_map.append(img_idx)\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(all_texts), batch_size), desc=\"Generating text embeddings\"):\n",
    "            batch_texts = all_texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize texts\n",
    "            text_tokens = tokenizer(batch_texts).to(device)\n",
    "            \n",
    "            # Get text embeddings\n",
    "            text_features = model.encode_text(text_tokens)\n",
    "            # Normalize embeddings\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            embeddings.append(text_features.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings), text_to_image_map\n",
    "\n",
    "# Generate or load text embeddings\n",
    "if text_embeddings_path.exists():\n",
    "    print(\"Loading existing text embeddings...\")\n",
    "    text_embeddings = np.load(text_embeddings_path)\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    text_to_image_map = metadata['text_to_image_map']\n",
    "else:\n",
    "    print(\"Generating text embeddings...\")\n",
    "    text_embeddings, text_to_image_map = generate_text_embeddings(slake_data, model, tokenizer, device)\n",
    "    np.save(text_embeddings_path, text_embeddings)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {'text_to_image_map': text_to_image_map}\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    print(f\"Text embeddings saved to {text_embeddings_path}\")\n",
    "\n",
    "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"Number of QA pairs: {len(text_to_image_map)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering Methods\n",
    "\n",
    "We'll apply four different clustering algorithms to the **image embeddings**:\n",
    "1. **DBSCAN**: Density-based clustering\n",
    "2. **HDBSCAN**: Hierarchical DBSCAN\n",
    "3. **K-means**: Centroid-based clustering\n",
    "4. **FINCH**: Parameter-free hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use image embeddings for clustering\n",
    "X = image_embeddings.copy()\n",
    "\n",
    "# Normalize embeddings (recommended for cosine-based models)\n",
    "from sklearn.preprocessing import normalize\n",
    "X_normalized = normalize(X, norm='l2')\n",
    "\n",
    "print(f\"Working with {X_normalized.shape[0]} image embeddings of dimension {X_normalized.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DBSCAN Clustering\n",
    "print(\"Running DBSCAN...\")\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')\n",
    "dbscan_labels = dbscan.fit_predict(X_normalized)\n",
    "\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise_dbscan = list(dbscan_labels).count(-1)\n",
    "print(f\"DBSCAN: {n_clusters_dbscan} clusters, {n_noise_dbscan} noise points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. HDBSCAN Clustering\n",
    "print(\"Running HDBSCAN...\")\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3)\n",
    "hdbscan_labels = hdbscan_clusterer.fit_predict(X_normalized)\n",
    "\n",
    "n_clusters_hdbscan = len(set(hdbscan_labels)) - (1 if -1 in hdbscan_labels else 0)\n",
    "n_noise_hdbscan = list(hdbscan_labels).count(-1)\n",
    "print(f\"HDBSCAN: {n_clusters_hdbscan} clusters, {n_noise_hdbscan} noise points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. K-means Clustering\n",
    "print(\"Running K-means...\")\n",
    "n_clusters_kmeans = 8  # Default reasonable value\n",
    "kmeans = KMeans(n_clusters=n_clusters_kmeans, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_normalized)\n",
    "\n",
    "print(f\"K-means: {n_clusters_kmeans} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. FINCH Clustering\n",
    "print(\"Running FINCH...\")\n",
    "finch_labels, _, _ = FINCH(X_normalized, verbose=False)\n",
    "# FINCH returns multiple partitions; we'll use the first one\n",
    "finch_labels = finch_labels[:, 0]\n",
    "\n",
    "n_clusters_finch = len(set(finch_labels))\n",
    "print(f\"FINCH: {n_clusters_finch} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of clustering results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLUSTERING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"DBSCAN:   {n_clusters_dbscan:3d} clusters, {n_noise_dbscan:3d} noise points\")\n",
    "print(f\"HDBSCAN:  {n_clusters_hdbscan:3d} clusters, {n_noise_hdbscan:3d} noise points\")\n",
    "print(f\"K-means:  {n_clusters_kmeans:3d} clusters\")\n",
    "print(f\"FINCH:    {n_clusters_finch:3d} clusters\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction\n",
    "\n",
    "Reduce the image embeddings to 2D using four different methods:\n",
    "1. **PCA**: Linear dimensionality reduction\n",
    "2. **t-SNE**: Non-linear, preserves local structure\n",
    "3. **UMAP**: Non-linear, preserves both local and global structure\n",
    "4. **h-NNE**: Hierarchical nearest neighbor embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PCA\n",
    "print(\"Running PCA...\")\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_embeddings = pca.fit_transform(X_normalized)\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. t-SNE\n",
    "print(\"Running t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "tsne_embeddings = tsne.fit_transform(X_normalized)\n",
    "print(\"t-SNE completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. UMAP\n",
    "print(\"Running UMAP...\")\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "umap_embeddings = umap_reducer.fit_transform(X_normalized)\n",
    "print(\"UMAP completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. h-NNE\n",
    "print(\"Running h-NNE...\")\n",
    "hnne_reducer = HNNE(n_components=2)\n",
    "hnne_embeddings = hnne_reducer.fit_transform(X_normalized)\n",
    "print(\"h-NNE completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization: Clustering Comparison on t-SNE\n",
    "\n",
    "Compare the four clustering methods by overlaying their labels on t-SNE embeddings in a 2x2 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x2 grid comparing clustering methods on t-SNE\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "fig.suptitle('Clustering Methods Comparison (t-SNE Embeddings)', fontsize=16, fontweight='bold')\n",
    "\n",
    "clustering_results = [\n",
    "    (dbscan_labels, 'DBSCAN', n_clusters_dbscan),\n",
    "    (hdbscan_labels, 'HDBSCAN', n_clusters_hdbscan),\n",
    "    (kmeans_labels, 'K-means', n_clusters_kmeans),\n",
    "    (finch_labels, 'FINCH', n_clusters_finch)\n",
    "]\n",
    "\n",
    "for idx, (labels, method_name, n_clusters) in enumerate(clustering_results):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Use different colors for each cluster\n",
    "    unique_labels = set(labels)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for label, color in zip(unique_labels, colors):\n",
    "        mask = labels == label\n",
    "        if label == -1:\n",
    "            # Noise points in black\n",
    "            ax.scatter(tsne_embeddings[mask, 0], tsne_embeddings[mask, 1], \n",
    "                      c='black', s=20, alpha=0.3, label='Noise')\n",
    "        else:\n",
    "            ax.scatter(tsne_embeddings[mask, 0], tsne_embeddings[mask, 1], \n",
    "                      c=[color], s=30, alpha=0.6, label=f'Cluster {label}')\n",
    "    \n",
    "    ax.set_title(f'{method_name} ({n_clusters} clusters)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('t-SNE 1', fontsize=11)\n",
    "    ax.set_ylabel('t-SNE 2', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Only show legend if not too many clusters\n",
    "    if len(unique_labels) <= 10:\n",
    "        ax.legend(loc='best', fontsize=8, ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Clustering comparison visualization saved as 'clustering_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Images per Cluster (K-means)\n",
    "\n",
    "Display 5 sample images from each K-means cluster. If a cluster has fewer than 5 images, fill with black placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_sample_grid(labels, image_paths, n_samples=5, img_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Create a grid showing sample images from each cluster.\n",
    "    \n",
    "    Args:\n",
    "        labels: Cluster labels for each image\n",
    "        image_paths: List of paths to images\n",
    "        n_samples: Number of samples to show per cluster\n",
    "        img_size: Size to resize images to\n",
    "    \"\"\"\n",
    "    unique_labels = sorted(set(labels))\n",
    "    # Filter out noise label if present\n",
    "    unique_labels = [l for l in unique_labels if l != -1]\n",
    "    \n",
    "    n_clusters = len(unique_labels)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_clusters, n_samples, figsize=(n_samples * 2, n_clusters * 2))\n",
    "    fig.suptitle(f'Sample Images per Cluster (K-means, {n_clusters} clusters)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Handle case of single cluster\n",
    "    if n_clusters == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for cluster_idx, label in enumerate(unique_labels):\n",
    "        # Get indices of images in this cluster\n",
    "        cluster_mask = labels == label\n",
    "        cluster_indices = np.where(cluster_mask)[0]\n",
    "        \n",
    "        # Sample up to n_samples images\n",
    "        n_available = len(cluster_indices)\n",
    "        sampled_indices = np.random.choice(cluster_indices, \n",
    "                                          size=min(n_samples, n_available), \n",
    "                                          replace=False)\n",
    "        \n",
    "        for sample_idx in range(n_samples):\n",
    "            ax = axes[cluster_idx, sample_idx]\n",
    "            \n",
    "            if sample_idx < len(sampled_indices):\n",
    "                # Load and display image\n",
    "                img_path = image_paths[sampled_indices[sample_idx]]\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img = img.resize(img_size)\n",
    "                ax.imshow(img)\n",
    "            else:\n",
    "                # Black filler image\n",
    "                ax.imshow(np.zeros((*img_size, 3), dtype=np.uint8))\n",
    "            \n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Add cluster label on first image\n",
    "            if sample_idx == 0:\n",
    "                ax.set_ylabel(f'Cluster {label}\\n({n_available} imgs)', \n",
    "                            fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cluster_samples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Cluster sample grid saved as 'cluster_samples.png'\")\n",
    "\n",
    "# Create image paths list\n",
    "image_paths = [sample['image_path'] for sample in slake_data]\n",
    "\n",
    "# Generate grid\n",
    "create_cluster_sample_grid(kmeans_labels, image_paths, n_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dimensionality Reduction Comparison\n",
    "\n",
    "Compare all four dimensionality reduction methods in a 2x2 grid, colored by K-means labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x2 grid comparing dimensionality reduction methods (colored by K-means)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "fig.suptitle('Dimensionality Reduction Methods Comparison (K-means Labels)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "reduction_results = [\n",
    "    (pca_embeddings, 'PCA'),\n",
    "    (tsne_embeddings, 't-SNE'),\n",
    "    (umap_embeddings, 'UMAP'),\n",
    "    (hnne_embeddings, 'h-NNE')\n",
    "]\n",
    "\n",
    "# Use same colors for K-means across all plots\n",
    "unique_kmeans = sorted(set(kmeans_labels))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_kmeans)))\n",
    "\n",
    "for idx, (embeddings, method_name) in enumerate(reduction_results):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Plot each cluster with consistent colors\n",
    "    for label, color in zip(unique_kmeans, colors):\n",
    "        mask = kmeans_labels == label\n",
    "        ax.scatter(embeddings[mask, 0], embeddings[mask, 1], \n",
    "                  c=[color], s=30, alpha=0.6, label=f'Cluster {label}')\n",
    "    \n",
    "    ax.set_title(f'{method_name}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(f'{method_name} 1', fontsize=11)\n",
    "    ax.set_ylabel(f'{method_name} 2', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='best', fontsize=8, ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dimensionality_reduction_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Dimensionality reduction comparison saved as 'dimensionality_reduction_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Outlier Detection (Bonus)\n",
    "\n",
    "We can leverage the clustering results for outlier detection:\n",
    "- **DBSCAN/HDBSCAN**: Noise points (label = -1) are outliers\n",
    "- **K-means**: Points far from cluster centers are potential outliers\n",
    "- **Isolation Forest**: Dedicated outlier detection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Isolation Forest for outlier detection\n",
    "print(\"Running Isolation Forest...\")\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "outlier_labels = iso_forest.fit_predict(X_normalized)\n",
    "# Convert to binary: -1 (outlier) and 1 (inlier)\n",
    "\n",
    "n_outliers_iso = list(outlier_labels).count(-1)\n",
    "print(f\"Isolation Forest detected {n_outliers_iso} outliers\")\n",
    "\n",
    "# Visualize outliers on t-SNE\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Outlier Detection Methods', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. DBSCAN outliers\n",
    "ax = axes[0]\n",
    "inliers = dbscan_labels != -1\n",
    "outliers = dbscan_labels == -1\n",
    "ax.scatter(tsne_embeddings[inliers, 0], tsne_embeddings[inliers, 1], \n",
    "          c='lightblue', s=30, alpha=0.5, label='Inliers')\n",
    "ax.scatter(tsne_embeddings[outliers, 0], tsne_embeddings[outliers, 1], \n",
    "          c='red', s=50, alpha=0.8, label='Outliers', marker='x')\n",
    "ax.set_title(f'DBSCAN ({n_noise_dbscan} outliers)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('t-SNE 1')\n",
    "ax.set_ylabel('t-SNE 2')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. HDBSCAN outliers\n",
    "ax = axes[1]\n",
    "inliers = hdbscan_labels != -1\n",
    "outliers = hdbscan_labels == -1\n",
    "ax.scatter(tsne_embeddings[inliers, 0], tsne_embeddings[inliers, 1], \n",
    "          c='lightblue', s=30, alpha=0.5, label='Inliers')\n",
    "ax.scatter(tsne_embeddings[outliers, 0], tsne_embeddings[outliers, 1], \n",
    "          c='red', s=50, alpha=0.8, label='Outliers', marker='x')\n",
    "ax.set_title(f'HDBSCAN ({n_noise_hdbscan} outliers)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('t-SNE 1')\n",
    "ax.set_ylabel('t-SNE 2')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Isolation Forest outliers\n",
    "ax = axes[2]\n",
    "inliers = outlier_labels == 1\n",
    "outliers = outlier_labels == -1\n",
    "ax.scatter(tsne_embeddings[inliers, 0], tsne_embeddings[inliers, 1], \n",
    "          c='lightblue', s=30, alpha=0.5, label='Inliers')\n",
    "ax.scatter(tsne_embeddings[outliers, 0], tsne_embeddings[outliers, 1], \n",
    "          c='red', s=50, alpha=0.8, label='Outliers', marker='x')\n",
    "ax.set_title(f'Isolation Forest ({n_outliers_iso} outliers)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('t-SNE 1')\n",
    "ax.set_ylabel('t-SNE 2')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outlier_detection.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Outlier detection visualization saved as 'outlier_detection.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Conclusions\n",
    "\n",
    "This tutorial demonstrated:\n",
    "\n",
    "### Embeddings\n",
    "- Generated medical image embeddings using BiomedCLIP\n",
    "- Created separate text embeddings for QA pairs\n",
    "- Saved embeddings as numpy files for reusability\n",
    "\n",
    "### Clustering Methods\n",
    "- **DBSCAN**: Density-based, identifies noise points\n",
    "- **HDBSCAN**: Hierarchical version, more robust to parameter choices\n",
    "- **K-means**: Simple, fast, requires predefined number of clusters\n",
    "- **FINCH**: Parameter-free, hierarchical\n",
    "\n",
    "### Dimensionality Reduction\n",
    "- **PCA**: Fast, linear, good for initial exploration\n",
    "- **t-SNE**: Excellent for visualization, preserves local structure\n",
    "- **UMAP**: Faster than t-SNE, preserves more global structure\n",
    "- **h-NNE**: Hierarchical, fast, structure-aware\n",
    "\n",
    "### Outlier Detection\n",
    "- Leveraged clustering noise points (DBSCAN/HDBSCAN)\n",
    "- Applied Isolation Forest for dedicated outlier detection\n",
    "\n",
    "### Key Takeaways\n",
    "1. Different methods reveal different aspects of data structure\n",
    "2. Clustering results vary significantly between methods\n",
    "3. Dimensionality reduction choice affects visual interpretation\n",
    "4. Medical imaging benefits from domain-specific embeddings (BiomedCLIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Experiments\n",
    "\n",
    "Try these experiments to deepen your understanding:\n",
    "\n",
    "1. **Different clustering parameters**: Adjust eps, min_samples for DBSCAN\n",
    "2. **Different K for K-means**: Try k=5, 10, 15 and compare\n",
    "3. **Text embeddings analysis**: Repeat clustering/reduction on QA embeddings\n",
    "4. **Combined embeddings**: Concatenate image + text embeddings\n",
    "5. **Other metrics**: Try clustering with cosine distance\n",
    "6. **Hierarchical visualization**: Explore FINCH hierarchy levels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
